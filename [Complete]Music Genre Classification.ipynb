{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Classification\n",
    "\n",
    "This notebook contains the proposed solution sketch for classification of musical genres.\n",
    "\n",
    "Pre-processing routines applied, feature extraction techniques and the models used for identification are enclosed in the notebook.\n",
    "\n",
    "Authors:\n",
    "Suryank Tiwari - MT19019\n",
    "Rose Verma - MT19052"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esteev\\Anaconda3\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import librosa\n",
    "import threading \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dataset Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GZTAN Dataset](http://marsyas.info/downloads/datasets.html) has been used for this project.\n",
    "\n",
    "This dataset contains <b>10 genres</b> in total and each genre has a 100 tracks in it for a <b>total 1000 tracks.</b>\n",
    "\n",
    "The tracks are all 22050Hz Mono 16-bit audio files in .wav format.\n",
    "\n",
    "This congrous nature of the dataset makes it a suitable choice for proceeding with the problem, but <font color='red'>the small size of the dataset restricts proper learning.</font>\n",
    "\n",
    "<b>This is resolved by creating non overlapping samples of shorter length from this dataset.</b> This increases the number of samples and decreases sample length for better representation while taking mean of features.\n",
    "\n",
    "<b><i>path</i></b> contains the path to original GZTAN dataset, and <b><i>out_path</i></b> should contain the path to an existing directory where the new oversampled dataset will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:\\IIITD\\Semester 2\\SML\\Project\\genres\\\\'\n",
    "out_path = 'E:\\IIITD\\Semester 2\\SML\\Project\\go\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTZAN dataset has the following structure.\n",
    "\n",
    "Main Dataset Directory: Genres\n",
    "\n",
    ">Genres<br>\n",
    ">>Blues<br>\n",
    ">>Classical<br>\n",
    ">>Country<br>\n",
    ">>Disco<br>\n",
    ">>Hiphop<br>\n",
    ">>Jazz<br>\n",
    ">>Metal<br>\n",
    ">>Pop<br>\n",
    ">>Reggae<br>\n",
    ">>Rock<br>\n",
    "\n",
    "The following snippet loads all the genres into <b><i>genre</i></b> list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre: ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
     ]
    }
   ],
   "source": [
    "genres = []\n",
    "for folder in os.listdir(path):\n",
    "    if os.path.isdir(path+folder):\n",
    "        genres.append(folder)\n",
    "print('Genre:', genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet splits each 30 second long tracks to multiple <b><i>sub_sample_length</i></b> samples.\n",
    "\n",
    "If <i> sub_sample_length = 5 </i> then each 30 second track is split into 6 non -overlapping sub tracks. Size of dataset becomes 6000 tracks in total with 600 tracks in each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sample_length = 5    # in seconds\n",
    "\n",
    "'''\n",
    "Don't alter the following parameters\n",
    "'''\n",
    "sample_length = 30      # Length of each track in GTZAN\n",
    "total_samples = 1000    # Number of tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet creates the oversampled dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled Dataset Processing Complete\n"
     ]
    }
   ],
   "source": [
    "for g in genres:\n",
    "    genrepath = path+'\\\\'+g\n",
    "    if not os.path.isdir(genrepath):\n",
    "        print('\\n', g, 'folder created')\n",
    "        os.mkdir(out_path+g)     \n",
    "        for filename in os.listdir(genrepath):\n",
    "            print('.', end='')\n",
    "            songname = genrepath+'\\\\'+filename\n",
    "            audio = AudioSegment.from_wav(songname)\n",
    "            n = len(audio) \n",
    "            partition = 1\n",
    "\n",
    "            interval = sub_sample_length * 1000  # n*1000 miliseconds\n",
    "            start = 0\n",
    "            end = interval\n",
    "\n",
    "            i=0\n",
    "            start = 0\n",
    "            end = interval \n",
    "            while i<sample_length*total_samples:\n",
    "                chunk = audio[start:end] \n",
    "\n",
    "                fln = filename.split('.')[1][-2:]+'_'+str(partition)+'.wav'          \n",
    "                chunk.export(out_path+g+'\\\\'+fln, format =\"wav\") \n",
    "\n",
    "                partition += 1\n",
    "                i+=interval\n",
    "\n",
    "                start = end\n",
    "                end = start + interval  \n",
    "                if end >= n: \n",
    "                    end = n \n",
    "print('Oversampled Dataset Processing Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampled dataset has been created at <i>out_path</i>. \n",
    "This dataset will now be used to extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following features have been explored using the <b>Librosa</b> library:\n",
    "* mfcc\n",
    "* chroma_stft\n",
    "* chroma_cens\n",
    "* chroma_cq\n",
    "* melspec\n",
    "* flatness\n",
    "* tempogram\n",
    "* poly_features order 0\n",
    "* poly_features order 1\n",
    "* poly_features order 2\n",
    "* spec_cent\n",
    "* spectral_contrast\n",
    "* spec_bw\n",
    "* rmse\n",
    "* rolloff\n",
    "* zcr\n",
    "* tonnetz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature listed above, we take the <b>mean</b>, <b>variance</b> and <b>standard deviation</b> of the feature value and add it to a CSV as our feature extraction policy.\n",
    "\n",
    "This section of the code can be slow since a lot of features are calculated, the code has been <font color='red'><b>multi-threaded</b></font> for <b>one thread per genre</b> to compute the features faster. Once computed, we can refer the CSV generated to access our features at a go.\n",
    "\n",
    "<b> Give the path to feature CSV below. If CSV has been generated once before, it will simply be loaded and won't be computed again. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'Features\\songdata5.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing features with mean, variance and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(features):\n",
    "    res = ''\n",
    "    for feature in features:\n",
    "        res+=str(np.mean(feature))+' '+str(np.var(feature))+' '+str(np.std(feature))+' '\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each thread runs concurrently on the following function. Snippet for genre feature computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genrepution(g, lock):\n",
    "    genrepath = out_path+'\\\\'+g\n",
    "    print(g)\n",
    "    for filename in os.listdir(genrepath):\n",
    "        print('.', end='')\n",
    "        songname = genrepath+'\\\\'+filename\n",
    "        y, sr = librosa.load(songname, mono=True)\n",
    "        \n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma_cens = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "        chroma_cq = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "        melspec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "        flatness = librosa.feature.spectral_flatness(y=y)\n",
    "        hop_length=512\n",
    "        oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n",
    "        tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr,hop_length=hop_length)\n",
    "        S = np.abs(librosa.stft(y))\n",
    "        p0 = librosa.feature.poly_features(S=S, order=0)\n",
    "        p1 = librosa.feature.poly_features(S=S, order=1)\n",
    "        p2 = librosa.feature.poly_features(S=S, order=2)\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        rmse = librosa.feature.rms(y=y)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        y = librosa.effects.harmonic(y)\n",
    "        tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "\n",
    "        to_append = filename+' '\n",
    "        features = [chroma_stft, chroma_cens, chroma_cq, melspec, flatness, tempogram, p0, p1, p2, spec_cent, spectral_contrast, spec_bw, rmse, rolloff, zcr, tonnetz]\n",
    "        to_append += process_features(features)\n",
    "        for e in mfcc:\n",
    "            to_append += str(np.mean(e))+' '+str(np.var(e))+' '+str(np.std(e))+' '\n",
    "        to_append += g\n",
    "        lock.acquire()\n",
    "        file = open(csv_file, 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(to_append.split())\n",
    "        lock.release()\n",
    "    print(g, 'finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the driver function that fills the dataframe with features using threads. The features are either computed or loaded from CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill():\n",
    "    if not os.path.isfile(csv_file):\n",
    "        \n",
    "        # Inserting column names into the CSV\n",
    "    \n",
    "        features = ['chroma_stft', 'chroma_cens', 'chroma_cq', 'melspec', 'flatness', 'tempogram', 'p0', 'p1', 'p2', 'spec_cent', 'spectral_contrast', 'spec_bw', 'rmse', 'rolloff', 'zcr', 'tonnetz']\n",
    "        col_names=[]\n",
    "        \n",
    "        total = len(features)*3+20*3+2\n",
    "        for i in range(total):\n",
    "            col_names.append(str(i))\n",
    "    \n",
    "        i=1\n",
    "        for f in features:\n",
    "            col_names[i]=f+'_mean'\n",
    "            col_names[i+1]=f+'_var'\n",
    "            col_names[i+2]=f+'_std'\n",
    "            i+=3\n",
    "        i=len(features)*3+1\n",
    "        j=0\n",
    "        while i+3<total:\n",
    "            col_names[i]='mfcc_'+str(j)+'_mean'\n",
    "            col_names[i+1]='mfcc_'+str(j)+'_var'\n",
    "            col_names[i+2]='mfcc_'+str(j)+'_std'\n",
    "            i+=3\n",
    "            j+=1\n",
    "        col_names[0] = 'filename'\n",
    "        col_names[-1] = 'label'\n",
    "        file = open(csv_file, 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(col_names)\n",
    "            \n",
    "        # Starting Feature Extraction Process\n",
    "        threads = []\n",
    "        lock = threading.Lock() \n",
    "        for g in genres:\n",
    "            t = threading.Thread(target=genrepution, args=(g, lock,)) \n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "            \n",
    "    # Loading the feature data file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    #df = df.sample(frac=1).reset_index(drop=True)   #Shuffling the dataset improves performance for RF\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section deals with classification and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 110)\n"
     ]
    }
   ],
   "source": [
    "data = fill()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply three techniques for feature selection: \n",
    "Feature Set Size = 110\n",
    "1. Variance Thresholding with threshold value 0.1: Feature Set Size = 84\n",
    "2. Remove Correlated Features : Feature Set Size = 56\n",
    "3. Apply F-ANOVA test using fclassif score function in Select KBest Algorithm : Feature Set Size = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 108) (1800, 108)\n",
      "Non Constant Features: 84\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(data['label'])\n",
    "X = data.drop(columns=[\"filename\", \"label\"])\n",
    "\n",
    "p_train, p_test, q_train, q_test = train_test_split(X, Y, shuffle=True, test_size=0.3, random_state=42)\n",
    "\n",
    "train = p_train.copy()\n",
    "#train['label'] = y_train \n",
    "\n",
    "test = p_test.copy()\n",
    "#test['label'] = y_test\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "zero_filter = VarianceThreshold(threshold=0.1)\n",
    "zero_filter.fit(train)\n",
    "\n",
    "features_left = train.columns[zero_filter.get_support()]\n",
    "print(\"Non Constant Features:\",len(features_left))\n",
    "train_filtered = pd.DataFrame(zero_filter.transform(train))\n",
    "test_filtered = pd.DataFrame(zero_filter.transform(test))\n",
    "\n",
    "train_filtered.columns = features_left\n",
    "test_filtered.columns = features_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Correlated Features and Apply F-ANOVA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after removal of correlated features:  56\n",
      "Final Feature Set:  Index(['melspec_mean', 'melspec_var', 'melspec_std', 'p0_mean', 'p0_var',\n",
      "       'p0_std', 'p1_var', 'spec_cent_mean', 'spec_cent_var',\n",
      "       'spectral_contrast_mean', 'spectral_contrast_var', 'spec_bw_mean',\n",
      "       'spec_bw_var', 'rolloff_var', 'mfcc_0_mean', 'mfcc_0_std',\n",
      "       'mfcc_1_mean', 'mfcc_1_var', 'mfcc_2_mean', 'mfcc_2_var', 'mfcc_3_mean',\n",
      "       'mfcc_3_var', 'mfcc_4_mean', 'mfcc_4_var', 'mfcc_5_mean', 'mfcc_5_var',\n",
      "       'mfcc_6_mean', 'mfcc_6_var', 'mfcc_7_mean', 'mfcc_7_var', 'mfcc_8_mean',\n",
      "       'mfcc_8_var', 'mfcc_9_mean', 'mfcc_9_var', 'mfcc_10_mean',\n",
      "       'mfcc_11_mean', 'mfcc_12_mean', 'mfcc_13_mean', 'mfcc_14_mean',\n",
      "       'mfcc_16_mean'],\n",
      "      dtype='object')\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "train = train_filtered.copy()\n",
    "train['label'] = q_train \n",
    "\n",
    "test = test_filtered.copy()\n",
    "test['label'] = q_test\n",
    "\n",
    "corr_matrix = train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "train = train.drop(data[to_drop], axis=1)\n",
    "test= test.drop(data[to_drop], axis=1)\n",
    "\n",
    "print(\"Features after removal of correlated features: \",len(train.columns))\n",
    "\n",
    "train_labels = train['label']\n",
    "test_labels = test['label']\n",
    "train = train.drop(columns= ['label'])\n",
    "test = test.drop(columns= ['label'])\n",
    "fvalue_selector = SelectKBest(f_classif, k=40).fit(train, train_labels)\n",
    "remaining_features = train.columns[fvalue_selector.get_support()]\n",
    "print(\"Final Feature Set: \",remaining_features)\n",
    "x_train = pd.DataFrame(fvalue_selector.transform(train))\n",
    "x_test = pd.DataFrame(fvalue_selector.transform(test))\n",
    "\n",
    "x_train.columns = remaining_features\n",
    "x_test.columns = remaining_features\n",
    "\n",
    "y_train = train_labels\n",
    "#x_train = Train_.drop(columns=['label'])\n",
    "\n",
    "y_test = test_labels\n",
    "#x_test = Test_.drop(columns=['label'])\n",
    "\n",
    "full_dataset= pd.concat([x_train, x_test], sort=False)\n",
    "print(len(full_dataset.columns))\n",
    "full_labels = pd.concat([y_train, y_test], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Esteev\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import seaborn as sns\n",
    "\n",
    "def draw_conf_mat(cm):\n",
    "    labels = label_encoder.inverse_transform(full_labels.unique())\n",
    "    sns.heatmap(cm, xticklabels=labels, yticklabels=labels, cmap=\"RdYlGn\", annot = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>clfs</b> is a list of classifiers to be applied to the problem. Train test split accuracy and cross validation score for 5 folds is computed and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b62863233e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclfs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    317\u001b[0m                     \u001b[0ml2_regularization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_regularization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                     shrinkage=self.learning_rate)\n\u001b[1;32m--> 319\u001b[1;33m                 \u001b[0mgrower\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 \u001b[0macc_apply_split_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrower\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_apply_split_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py\u001b[0m in \u001b[0;36mgrow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;34m\"\"\"Grow the tree, from root to leaves.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplittable_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_intilialize_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessians\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessians_are_constant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\grower.py\u001b[0m in \u001b[0;36msplit_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[0msmallest_child\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistograms\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m                 self.histogram_builder.compute_histograms_brute(\n\u001b[1;32m--> 398\u001b[1;33m                     smallest_child.sample_indices)\n\u001b[0m\u001b[0;32m    399\u001b[0m             \u001b[0mlargest_child\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistograms\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m                 self.histogram_builder.compute_histograms_subtraction(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "num_trees = 4000\n",
    "max_features='sqrt'\n",
    "max_depth=12\n",
    "criterion='entropy'\n",
    "seed      = 9\n",
    "\n",
    "clfs = [#XGBClassifier(), \n",
    "        #GaussianNB(), \n",
    "        #BernoulliNB(), \n",
    "        #LGBMClassifier(objective='multiclass', random_state=9),\n",
    "        #LogisticRegression(max_iter=1000),  \n",
    "        #AdaBoostClassifier(n_estimators=1000),\n",
    "        #svm.SVC(decision_function_shape='ovo') ,\n",
    "        #svm.LinearSVC(),\n",
    "        #GradientBoostingClassifier(n_estimators=1000, random_state=0),\n",
    "        HistGradientBoostingClassifier(l2_regularization=0.1, learning_rate=0.1,\n",
    "                               loss='auto', max_bins=255, max_depth=12,\n",
    "                               max_iter=1000, max_leaf_nodes=31,\n",
    "                               min_samples_leaf=20, n_iter_no_change=None,\n",
    "                               random_state=42, scoring=None, tol=1e-07,\n",
    "                               validation_fraction=0.1, verbose=0,\n",
    "                               warm_start=False),\n",
    "        #BaggingClassifier(XGBClassifier(), max_samples=0.5), \n",
    "        RandomForestClassifier(n_estimators=num_trees, random_state=seed, criterion=criterion, max_features=max_features, max_depth=max_depth)\n",
    "        ]\n",
    "\n",
    "for clf in clfs:\n",
    "    clf.fit(x_train, y_train)\n",
    "    prediction = clf.predict(x_test)\n",
    "    \n",
    "    print(\"\\n\",clf)\n",
    "    print('Testing Accuracy :%.3f' % accuracy_score(prediction, y_test))\n",
    "    cm = confusion_matrix(y_test, prediction)\n",
    "    draw_conf_mat(cm)\n",
    "    scores = cross_val_score(clf, full_dataset,full_labels , cv=3)\n",
    "    print('Cross Validation Accuracy :%.3f' % np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
